{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danadorn/404-/blob/main/Lab_2_Data_Preprocessing_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXL8uJMv0BL1"
      },
      "source": [
        "# Lab 2: Data Preprocessing for Machine Learning\n",
        "**Course:** Fundamentals of Machine Learning  \n",
        "**Instructor:** Mr. HIM Soklong  \n",
        "**Student Name:** Sok  \n",
        "**Student ID:** CSE00000\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this lab, you will be able to:\n",
        "1. Load and explore datasets using pandas\n",
        "2. Identify different types of data and variables\n",
        "3. Handle missing values effectively\n",
        "4. Detect and handle outliers using Z-score and IQR methods\n",
        "5. Apply feature scaling (Standardization and Normalization)\n",
        "6. Encode categorical variables\n",
        "7. Analyze feature correlations\n",
        "8. Handle imbalanced datasets\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1gn8fX10BL3"
      },
      "source": [
        "## Part 0: Introduction to Pandas\n",
        "\n",
        "Pandas is the most important library for data manipulation in Python. Let's learn the basics!\n",
        "\n",
        "### 0.1 Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvZlrZpr0BL3",
        "outputId": "bf0f7d8d-3872-4423-d218-12a3bffe3ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n",
            "Pandas version: 2.2.2\n",
            "NumPy version: 2.0.2\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJwCbkhu0BL4"
      },
      "source": [
        "### 0.2 Creating DataFrames\n",
        "\n",
        "A DataFrame is a 2-dimensional labeled data structure with columns of potentially different types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMs1aHyy0BL4",
        "outputId": "15cc2be1-0649-47a6-aa15-33ee6b090313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample DataFrame:\n",
            "      Name  Age        City  Salary\n",
            "0    Alice   25  Phnom Penh   50000\n",
            "1      Bob   30   Siem Reap   60000\n",
            "2  Charlie   35  Phnom Penh   75000\n",
            "3    David   28  Battambang   55000\n",
            "4      Eve   32   Siem Reap   70000\n",
            "\n",
            "DataFrame shape: (5, 4)\n",
            "DataFrame columns: ['Name', 'Age', 'City', 'Salary']\n"
          ]
        }
      ],
      "source": [
        "# Create a simple DataFrame from a dictionary\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
        "    'Age': [25, 30, 35, 28, 32],\n",
        "    'City': ['Phnom Penh', 'Siem Reap', 'Phnom Penh', 'Battambang', 'Siem Reap'],\n",
        "    'Salary': [50000, 60000, 75000, 55000, 70000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Sample DataFrame:\")\n",
        "print(df)\n",
        "print(\"\\nDataFrame shape:\", df.shape)  # (rows, columns)\n",
        "print(\"DataFrame columns:\", df.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKtXlgvS0BL4"
      },
      "source": [
        "### 0.3 Basic DataFrame Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHdPGp5N0BL4",
        "outputId": "b6202cfd-529f-407b-c0a2-3b6355c932a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 rows:\n",
            "      Name  Age        City  Salary\n",
            "0    Alice   25  Phnom Penh   50000\n",
            "1      Bob   30   Siem Reap   60000\n",
            "2  Charlie   35  Phnom Penh   75000\n",
            "\n",
            "Last 2 rows:\n",
            "    Name  Age        City  Salary\n",
            "3  David   28  Battambang   55000\n",
            "4    Eve   32   Siem Reap   70000\n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5 entries, 0 to 4\n",
            "Data columns (total 4 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Name    5 non-null      object\n",
            " 1   Age     5 non-null      int64 \n",
            " 2   City    5 non-null      object\n",
            " 3   Salary  5 non-null      int64 \n",
            "dtypes: int64(2), object(2)\n",
            "memory usage: 292.0+ bytes\n",
            "None\n",
            "\n",
            "Statistical Summary:\n",
            "             Age        Salary\n",
            "count   5.000000      5.000000\n",
            "mean   30.000000  62000.000000\n",
            "std     3.807887  10368.220677\n",
            "min    25.000000  50000.000000\n",
            "25%    28.000000  55000.000000\n",
            "50%    30.000000  60000.000000\n",
            "75%    32.000000  70000.000000\n",
            "max    35.000000  75000.000000\n"
          ]
        }
      ],
      "source": [
        "# Display first few rows\n",
        "print(\"First 3 rows:\")\n",
        "print(df.head(3))\n",
        "\n",
        "# Display last few rows\n",
        "print(\"\\nLast 2 rows:\")\n",
        "print(df.tail(2))\n",
        "\n",
        "# Get DataFrame information\n",
        "print(\"\\nDataFrame Info:\")\n",
        "print(df.info())\n",
        "\n",
        "# Statistical summary\n",
        "print(\"\\nStatistical Summary:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqigdtxs0BL4"
      },
      "source": [
        "### 0.4 Selecting Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SJlvL9X0BL4",
        "outputId": "c57e0303-0d57-4c0d-e684-2d85abe40189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Age column:\n",
            "0    25\n",
            "1    30\n",
            "2    35\n",
            "3    28\n",
            "4    32\n",
            "Name: Age, dtype: int64\n",
            "<class 'pandas.core.series.Series'>\n",
            "\n",
            "Name and Salary columns:\n",
            "      Name  Salary\n",
            "0    Alice   50000\n",
            "1      Bob   60000\n",
            "2  Charlie   75000\n",
            "3    David   55000\n",
            "4      Eve   70000\n",
            "\n",
            "Row at index 2:\n",
            "Name         Charlie\n",
            "Age               35\n",
            "City      Phnom Penh\n",
            "Salary         75000\n",
            "Name: 2, dtype: object\n",
            "\n",
            "People older than 30:\n",
            "      Name  Age        City  Salary\n",
            "2  Charlie   35  Phnom Penh   75000\n",
            "4      Eve   32   Siem Reap   70000\n",
            "\n",
            "Age and City for first 3 people:\n",
            "   Age        City\n",
            "0   25  Phnom Penh\n",
            "1   30   Siem Reap\n",
            "2   35  Phnom Penh\n"
          ]
        }
      ],
      "source": [
        "# Select a single column (returns a Series)\n",
        "print(\"Age column:\")\n",
        "print(df['Age'])\n",
        "print(type(df['Age']))\n",
        "\n",
        "# Select multiple columns (returns a DataFrame)\n",
        "print(\"\\nName and Salary columns:\")\n",
        "print(df[['Name', 'Salary']])\n",
        "\n",
        "# Select rows by index\n",
        "print(\"\\nRow at index 2:\")\n",
        "print(df.iloc[2])  # iloc = integer location\n",
        "\n",
        "# Select rows by condition\n",
        "print(\"\\nPeople older than 30:\")\n",
        "print(df[df['Age'] > 30])\n",
        "\n",
        "# Select specific rows and columns\n",
        "print(\"\\nAge and City for first 3 people:\")\n",
        "print(df.loc[0:2, ['Age', 'City']])  # loc = label location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANqgwu440BL5"
      },
      "source": [
        "### 0.5 Exercise: Pandas Basics\n",
        "\n",
        "**Task:** Complete the following exercises using the DataFrame above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc4pRqM50BL5"
      },
      "outputs": [],
      "source": [
        "# Exercise 1: Select all people from Phnom Penh\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# Exercise 2: Calculate the average salary\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# Exercise 3: Find the person with the highest salary\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# Exercise 4: Create a new column 'Experience' with random values between 1-10\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBd8GSva0BL5"
      },
      "source": [
        "---\n",
        "## Part 1: Loading and Exploring Real Data\n",
        "\n",
        "Let's work with a real dataset. We'll create a sample customer dataset for practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW7xl4zr0BL5"
      },
      "outputs": [],
      "source": [
        "# Create a sample customer dataset\n",
        "np.random.seed(42)\n",
        "\n",
        "n_samples = 100\n",
        "\n",
        "customer_data = {\n",
        "    'CustomerID': range(1, n_samples + 1),\n",
        "    'Age': np.random.randint(18, 70, n_samples),\n",
        "    'Income': np.random.normal(50000, 20000, n_samples),\n",
        "    'SpendingScore': np.random.randint(1, 100, n_samples),\n",
        "    'Gender': np.random.choice(['Male', 'Female'], n_samples),\n",
        "    'Education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples),\n",
        "    'City': np.random.choice(['Phnom Penh', 'Siem Reap', 'Battambang', 'Kampot'], n_samples),\n",
        "    'Purchased': np.random.choice([0, 1], n_samples, p=[0.7, 0.3])  # Imbalanced\n",
        "}\n",
        "\n",
        "# Add some missing values intentionally\n",
        "customer_data['Income'][np.random.choice(n_samples, 10, replace=False)] = np.nan\n",
        "customer_data['Age'][np.random.choice(n_samples, 5, replace=False)] = np.nan\n",
        "\n",
        "# Add some outliers\n",
        "customer_data['Income'][0] = 200000  # Outlier\n",
        "customer_data['Age'][1] = 5  # Outlier\n",
        "\n",
        "df_customers = pd.DataFrame(customer_data)\n",
        "\n",
        "# Save to CSV\n",
        "df_customers.to_csv('customer_data.csv', index=False)\n",
        "\n",
        "print(\"Customer dataset created!\")\n",
        "print(f\"Shape: {df_customers.shape}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "df_customers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix_15uWo0BL5"
      },
      "source": [
        "### 1.1 Loading Data from CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgjYl0fT0BL5"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('customer_data.csv')\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Display first few rows\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfcLGnDB0BL5"
      },
      "source": [
        "### 1.2 Identifying Data Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdCypXfk0BL5"
      },
      "outputs": [],
      "source": [
        "# Check data types\n",
        "print(\"Data Types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Identify numerical and categorical columns\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"\\nNumerical columns: {numerical_cols}\")\n",
        "print(f\"Categorical columns: {categorical_cols}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzCwDDeE0BL5"
      },
      "source": [
        "### 1.3 Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpaDwGI_0BL5"
      },
      "outputs": [],
      "source": [
        "# Statistical summary for numerical features\n",
        "print(\"Statistical Summary:\")\n",
        "df[numerical_cols].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYgzQV6r0BL5"
      },
      "outputs": [],
      "source": [
        "# Value counts for categorical features\n",
        "for col in categorical_cols:\n",
        "    print(f\"\\n{col} value counts:\")\n",
        "    print(df[col].value_counts())\n",
        "    print(f\"Unique values: {df[col].nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIT6tfVq0BL5"
      },
      "outputs": [],
      "source": [
        "# Visualize numerical features\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Distribution of Numerical Features', fontsize=16)\n",
        "\n",
        "numerical_features = ['Age', 'Income', 'SpendingScore']\n",
        "for idx, col in enumerate(numerical_features):\n",
        "    row = idx // 2\n",
        "    col_idx = idx % 2\n",
        "    axes[row, col_idx].hist(df[col].dropna(), bins=20, edgecolor='black', alpha=0.7)\n",
        "    axes[row, col_idx].set_title(f'{col} Distribution')\n",
        "    axes[row, col_idx].set_xlabel(col)\n",
        "    axes[row, col_idx].set_ylabel('Frequency')\n",
        "\n",
        "# Box plot for outlier detection\n",
        "axes[1, 1].boxplot([df['Age'].dropna(), df['Income'].dropna()], labels=['Age', 'Income'])\n",
        "axes[1, 1].set_title('Box Plot for Outlier Detection')\n",
        "axes[1, 1].set_ylabel('Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT2pViKF0BL5"
      },
      "source": [
        "---\n",
        "## Part 2: Handling Missing Values\n",
        "\n",
        "Missing values can significantly impact model performance. Let's learn how to detect and handle them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxQgHTt60BL5"
      },
      "source": [
        "### 2.1 Detecting Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4d9y-Fd0BL5"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing Values Count:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nMissing Values Percentage:\")\n",
        "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
        "print(missing_pct[missing_pct > 0])\n",
        "\n",
        "# Visualize missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cmap='viridis', cbar=True, yticklabels=False)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onDgJHo-0BL6"
      },
      "source": [
        "### 2.2 Handling Missing Values\n",
        "\n",
        "**Common strategies:**\n",
        "1. **Delete rows/columns** - Only if missing data is minimal (<5%)\n",
        "2. **Imputation** - Replace with:\n",
        "   - **Mean** - for normally distributed data\n",
        "   - **Median** - for skewed data or when outliers exist\n",
        "   - **Mode** - for categorical data\n",
        "   - **Forward/Backward fill** - for time series data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z31F5Pw0BL6"
      },
      "outputs": [],
      "source": [
        "# Make a copy of the dataframe\n",
        "df_cleaned = df.copy()\n",
        "\n",
        "# Method 1: Fill numerical columns with median (better for data with outliers)\n",
        "df_cleaned['Age'].fillna(df_cleaned['Age'].median(), inplace=True)\n",
        "df_cleaned['Income'].fillna(df_cleaned['Income'].median(), inplace=True)\n",
        "\n",
        "# Verify\n",
        "print(\"Missing values after imputation:\")\n",
        "print(df_cleaned.isnull().sum())\n",
        "\n",
        "# Compare statistics before and after\n",
        "print(\"\\nAge - Before and After Imputation:\")\n",
        "print(f\"Original mean: {df['Age'].mean():.2f}\")\n",
        "print(f\"Cleaned mean: {df_cleaned['Age'].mean():.2f}\")\n",
        "print(f\"Original median: {df['Age'].median():.2f}\")\n",
        "print(f\"Cleaned median: {df_cleaned['Age'].median():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbXDo1M40BL6"
      },
      "source": [
        "### 2.3 Exercise: Missing Value Imputation\n",
        "\n",
        "**Task:** Create a new dataset with missing values and apply different imputation strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_-Rxetz0BL6"
      },
      "outputs": [],
      "source": [
        "# Create test data with missing values\n",
        "test_data = {\n",
        "    'Score1': [85, 90, np.nan, 78, 92, np.nan, 88, 95],\n",
        "    'Score2': [75, np.nan, 82, 79, np.nan, 85, 90, 88],\n",
        "    'Category': ['A', 'B', np.nan, 'A', 'B', 'C', np.nan, 'A']\n",
        "}\n",
        "df_test = pd.DataFrame(test_data)\n",
        "\n",
        "print(\"Original data:\")\n",
        "print(df_test)\n",
        "\n",
        "# TODO: Exercise 1 - Fill Score1 with mean\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 2 - Fill Score2 with median\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 3 - Fill Category with mode\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 4 - Drop rows with any missing values\n",
        "# df_test_dropped = ...\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkWksyyw0BL6"
      },
      "source": [
        "---\n",
        "## Part 3: Detecting and Handling Outliers\n",
        "\n",
        "Outliers are data points that are significantly different from other observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ4tsbcM0BL6"
      },
      "source": [
        "### 3.1 Method 1: Z-Score Method\n",
        "\n",
        "**Formula:** $z = \\frac{(x - \\mu)}{\\sigma}$\n",
        "\n",
        "**Rule:** Data points with $|z| > 3$ are considered outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VBgOUqT0BL6"
      },
      "outputs": [],
      "source": [
        "def detect_outliers_zscore(data, threshold=3):\n",
        "    \"\"\"\n",
        "    Detect outliers using Z-score method\n",
        "\n",
        "    Parameters:\n",
        "    data: pandas Series\n",
        "    threshold: Z-score threshold (default=3)\n",
        "\n",
        "    Returns:\n",
        "    Boolean mask where True indicates outlier\n",
        "    \"\"\"\n",
        "    z_scores = np.abs(stats.zscore(data.dropna()))\n",
        "    return z_scores > threshold\n",
        "\n",
        "# Detect outliers in Age\n",
        "print(\"=== Age Outlier Detection (Z-Score) ===\")\n",
        "age_data = df_cleaned['Age'].dropna()\n",
        "age_mean = age_data.mean()\n",
        "age_std = age_data.std()\n",
        "\n",
        "print(f\"Mean: {age_mean:.2f}\")\n",
        "print(f\"Std: {age_std:.2f}\")\n",
        "print(f\"Threshold: Â±{3 * age_std:.2f} from mean\")\n",
        "\n",
        "# Calculate z-scores\n",
        "z_scores = np.abs(stats.zscore(age_data))\n",
        "outliers = z_scores > 3\n",
        "\n",
        "print(f\"\\nOutliers found: {outliers.sum()}\")\n",
        "if outliers.sum() > 0:\n",
        "    print(\"Outlier values:\")\n",
        "    print(age_data[outliers])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzlvCHmt0BL6"
      },
      "outputs": [],
      "source": [
        "# Detect outliers in Income\n",
        "print(\"=== Income Outlier Detection (Z-Score) ===\")\n",
        "income_data = df_cleaned['Income'].dropna()\n",
        "income_mean = income_data.mean()\n",
        "income_std = income_data.std()\n",
        "\n",
        "print(f\"Mean: ${income_mean:.2f}\")\n",
        "print(f\"Std: ${income_std:.2f}\")\n",
        "\n",
        "z_scores = np.abs(stats.zscore(income_data))\n",
        "outliers = z_scores > 3\n",
        "\n",
        "print(f\"\\nOutliers found: {outliers.sum()}\")\n",
        "if outliers.sum() > 0:\n",
        "    print(\"Outlier values:\")\n",
        "    print(income_data[outliers])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLGMuymd0BL6"
      },
      "source": [
        "### 3.2 Method 2: IQR (Interquartile Range) Method\n",
        "\n",
        "**Formula:**\n",
        "- $IQR = Q3 - Q1$\n",
        "- Lower Bound = $Q1 - 1.5 \\times IQR$\n",
        "- Upper Bound = $Q3 + 1.5 \\times IQR$\n",
        "\n",
        "**Rule:** Data points outside [Lower Bound, Upper Bound] are outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KeZKNoP0BL6"
      },
      "outputs": [],
      "source": [
        "def detect_outliers_iqr(data):\n",
        "    \"\"\"\n",
        "    Detect outliers using IQR method\n",
        "\n",
        "    Parameters:\n",
        "    data: pandas Series\n",
        "\n",
        "    Returns:\n",
        "    Boolean mask where True indicates outlier\n",
        "    \"\"\"\n",
        "    Q1 = data.quantile(0.25)\n",
        "    Q3 = data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    return (data < lower_bound) | (data > upper_bound)\n",
        "\n",
        "# Detect outliers in Age using IQR\n",
        "print(\"=== Age Outlier Detection (IQR) ===\")\n",
        "age_data = df_cleaned['Age'].dropna()\n",
        "\n",
        "Q1 = age_data.quantile(0.25)\n",
        "Q2 = age_data.quantile(0.50)  # Median\n",
        "Q3 = age_data.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "print(f\"Q1 (25th percentile): {Q1:.2f}\")\n",
        "print(f\"Q2 (Median): {Q2:.2f}\")\n",
        "print(f\"Q3 (75th percentile): {Q3:.2f}\")\n",
        "print(f\"IQR: {IQR:.2f}\")\n",
        "print(f\"Lower Bound: {lower_bound:.2f}\")\n",
        "print(f\"Upper Bound: {upper_bound:.2f}\")\n",
        "\n",
        "outliers = (age_data < lower_bound) | (age_data > upper_bound)\n",
        "print(f\"\\nOutliers found: {outliers.sum()}\")\n",
        "if outliers.sum() > 0:\n",
        "    print(\"Outlier values:\")\n",
        "    print(age_data[outliers])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGQDdozh0BL6"
      },
      "outputs": [],
      "source": [
        "# Visualize outliers with box plot\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Age\n",
        "axes[0].boxplot(df_cleaned['Age'].dropna())\n",
        "axes[0].set_title('Age Box Plot')\n",
        "axes[0].set_ylabel('Age')\n",
        "\n",
        "# Income\n",
        "axes[1].boxplot(df_cleaned['Income'].dropna())\n",
        "axes[1].set_title('Income Box Plot')\n",
        "axes[1].set_ylabel('Income ($)')\n",
        "\n",
        "# SpendingScore\n",
        "axes[2].boxplot(df_cleaned['SpendingScore'].dropna())\n",
        "axes[2].set_title('Spending Score Box Plot')\n",
        "axes[2].set_ylabel('Spending Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP-h8tFs0BL6"
      },
      "source": [
        "### 3.3 Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9tVcuXz0BL6"
      },
      "outputs": [],
      "source": [
        "# Method 1: Remove outliers\n",
        "df_no_outliers = df_cleaned.copy()\n",
        "\n",
        "# Remove Age outliers using IQR\n",
        "age_outliers = detect_outliers_iqr(df_no_outliers['Age'])\n",
        "income_outliers = detect_outliers_iqr(df_no_outliers['Income'])\n",
        "\n",
        "print(f\"Rows before removing outliers: {len(df_no_outliers)}\")\n",
        "df_no_outliers = df_no_outliers[~(age_outliers | income_outliers)]\n",
        "print(f\"Rows after removing outliers: {len(df_no_outliers)}\")\n",
        "print(f\"Rows removed: {len(df_cleaned) - len(df_no_outliers)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKR0W8Zg0BL6"
      },
      "outputs": [],
      "source": [
        "# Method 2: Cap outliers (Winsorization)\n",
        "df_capped = df_cleaned.copy()\n",
        "\n",
        "# Cap Income outliers\n",
        "Q1 = df_capped['Income'].quantile(0.25)\n",
        "Q3 = df_capped['Income'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Cap values\n",
        "df_capped['Income'] = df_capped['Income'].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "print(\"Income statistics after capping:\")\n",
        "print(df_capped['Income'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUtEAkl-0BL6"
      },
      "source": [
        "### 3.4 Exercise: Outlier Detection\n",
        "\n",
        "**Task:** Practice outlier detection with the following data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKcTsASV0BMD"
      },
      "outputs": [],
      "source": [
        "# Test data\n",
        "test_scores = np.array([55, 48, 44, 98, 51, 39, 10, 62, 58, 52, 47])\n",
        "\n",
        "print(\"Test Scores:\", test_scores)\n",
        "print(f\"Mean: {test_scores.mean():.2f}\")\n",
        "print(f\"Std: {test_scores.std():.2f}\")\n",
        "\n",
        "# TODO: Exercise 1 - Detect outliers using Z-score method (threshold=2)\n",
        "# Hint: Calculate z-scores and find which values have |z| > 2\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 2 - Detect outliers using IQR method\n",
        "# Hint: Calculate Q1, Q3, IQR, and bounds\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 3 - Remove outliers and calculate new mean\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtbrYEFU0BMD"
      },
      "source": [
        "---\n",
        "## Part 4: Feature Scaling\n",
        "\n",
        "Feature scaling brings all features to the same scale, which is crucial for many ML algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeOfIboE0BMD"
      },
      "source": [
        "### 4.1 Standardization (Z-score Normalization)\n",
        "\n",
        "**Formula:** $x_{new} = \\frac{x - \\mu}{\\sigma}$\n",
        "\n",
        "**Result:** Mean = 0, Standard Deviation = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvJgE9Pv0BMD"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select features for scaling\n",
        "features_to_scale = ['Age', 'Income', 'SpendingScore']\n",
        "df_for_scaling = df_no_outliers[features_to_scale].copy()\n",
        "\n",
        "# Create scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform\n",
        "df_standardized = pd.DataFrame(\n",
        "    scaler.fit_transform(df_for_scaling),\n",
        "    columns=[f'{col}_standardized' for col in features_to_scale],\n",
        "    index=df_for_scaling.index\n",
        ")\n",
        "\n",
        "# Compare original and standardized\n",
        "print(\"Original Data Statistics:\")\n",
        "print(df_for_scaling.describe())\n",
        "\n",
        "print(\"\\nStandardized Data Statistics:\")\n",
        "print(df_standardized.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ_MhkA40BMD"
      },
      "outputs": [],
      "source": [
        "# Visualize the effect of standardization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "fig.suptitle('Before and After Standardization', fontsize=16)\n",
        "\n",
        "for idx, col in enumerate(features_to_scale):\n",
        "    # Original\n",
        "    axes[0, idx].hist(df_for_scaling[col], bins=20, edgecolor='black', alpha=0.7)\n",
        "    axes[0, idx].set_title(f'{col} (Original)')\n",
        "    axes[0, idx].set_xlabel(col)\n",
        "    axes[0, idx].set_ylabel('Frequency')\n",
        "\n",
        "    # Standardized\n",
        "    axes[1, idx].hist(df_standardized[f'{col}_standardized'], bins=20, edgecolor='black', alpha=0.7, color='orange')\n",
        "    axes[1, idx].set_title(f'{col} (Standardized)')\n",
        "    axes[1, idx].set_xlabel(f'{col}_standardized')\n",
        "    axes[1, idx].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLve0Xbo0BMD"
      },
      "source": [
        "### 4.2 Min-Max Normalization\n",
        "\n",
        "**Formula:** $x_{new} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
        "\n",
        "**Result:** Values scaled to [0, 1] range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTNK87mg0BMD"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create scaler\n",
        "minmax_scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform\n",
        "df_normalized = pd.DataFrame(\n",
        "    minmax_scaler.fit_transform(df_for_scaling),\n",
        "    columns=[f'{col}_normalized' for col in features_to_scale],\n",
        "    index=df_for_scaling.index\n",
        ")\n",
        "\n",
        "print(\"Normalized Data Statistics:\")\n",
        "print(df_normalized.describe())\n",
        "\n",
        "# Verify the range is [0, 1]\n",
        "print(\"\\nMin and Max values:\")\n",
        "for col in df_normalized.columns:\n",
        "    print(f\"{col}: Min={df_normalized[col].min():.4f}, Max={df_normalized[col].max():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWO03XXD0BMD"
      },
      "source": [
        "### 4.3 Exercise: Feature Scaling\n",
        "\n",
        "**Task:** Apply both scaling methods and compare results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJhKS9DH0BMD"
      },
      "outputs": [],
      "source": [
        "# Test data - different scales\n",
        "test_data = pd.DataFrame({\n",
        "    'Height_cm': [150, 160, 170, 180, 190],\n",
        "    'Weight_kg': [50, 60, 70, 80, 90],\n",
        "    'Income_USD': [30000, 40000, 50000, 60000, 70000]\n",
        "})\n",
        "\n",
        "print(\"Original Data:\")\n",
        "print(test_data)\n",
        "\n",
        "# TODO: Exercise 1 - Apply standardization\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 2 - Apply min-max normalization\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 3 - Compare the mean and std of standardized data\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 4 - Verify normalized data is in [0,1] range\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDn59ReG0BMD"
      },
      "source": [
        "---\n",
        "## Part 5: Feature Encoding\n",
        "\n",
        "Converting categorical variables into numerical format for ML algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCCGX0Si0BMD"
      },
      "source": [
        "### 5.1 Label Encoding\n",
        "\n",
        "Assigns a unique integer to each category. **Use only for ordinal data!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u04JXanz0BMD"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Education has natural ordering: High School < Bachelor < Master < PhD\n",
        "df_encoded = df_no_outliers.copy()\n",
        "\n",
        "# Manual mapping for ordinal data\n",
        "education_mapping = {\n",
        "    'High School': 0,\n",
        "    'Bachelor': 1,\n",
        "    'Master': 2,\n",
        "    'PhD': 3\n",
        "}\n",
        "\n",
        "df_encoded['Education_encoded'] = df_encoded['Education'].map(education_mapping)\n",
        "\n",
        "print(\"Original vs Encoded Education:\")\n",
        "print(df_encoded[['Education', 'Education_encoded']].drop_duplicates().sort_values('Education_encoded'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPCilyCp0BMD"
      },
      "outputs": [],
      "source": [
        "# Using LabelEncoder (automatic, but doesn't preserve order)\n",
        "label_encoder = LabelEncoder()\n",
        "df_encoded['Gender_encoded'] = label_encoder.fit_transform(df_encoded['Gender'])\n",
        "\n",
        "print(\"\\nGender Encoding:\")\n",
        "print(df_encoded[['Gender', 'Gender_encoded']].drop_duplicates())\n",
        "\n",
        "# Get the mapping\n",
        "print(\"\\nEncoding mapping:\")\n",
        "for idx, label in enumerate(label_encoder.classes_):\n",
        "    print(f\"{label}: {idx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Js0uZu0BMD"
      },
      "source": [
        "### 5.2 One-Hot Encoding\n",
        "\n",
        "Creates binary columns for each category. **Use for nominal data (no natural order).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rEJe4_y0BMD"
      },
      "outputs": [],
      "source": [
        "# One-hot encode City (nominal data - no natural order)\n",
        "city_encoded = pd.get_dummies(df_encoded['City'], prefix='City')\n",
        "\n",
        "print(\"One-Hot Encoded City:\")\n",
        "print(city_encoded.head(10))\n",
        "\n",
        "# Concatenate with original dataframe\n",
        "df_encoded = pd.concat([df_encoded, city_encoded], axis=1)\n",
        "\n",
        "print(\"\\nDataFrame with one-hot encoded features:\")\n",
        "print(df_encoded[['City'] + list(city_encoded.columns)].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVn9ehYp0BMD"
      },
      "outputs": [],
      "source": [
        "# Alternative: Using scikit-learn\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' to avoid dummy variable trap\n",
        "gender_encoded = onehot_encoder.fit_transform(df_encoded[['Gender']])\n",
        "\n",
        "print(\"One-Hot Encoded Gender (with drop='first'):\")\n",
        "print(\"Categories:\", onehot_encoder.categories_)\n",
        "print(\"\\nEncoded values (first 5 rows):\")\n",
        "print(gender_encoded[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6cirUhB0BME"
      },
      "source": [
        "### 5.3 Exercise: Feature Encoding\n",
        "\n",
        "**Task:** Practice encoding categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SG-cUXY0BME"
      },
      "outputs": [],
      "source": [
        "# Test data\n",
        "student_data = pd.DataFrame({\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
        "    'Grade': ['A', 'B', 'A', 'C'],  # Ordinal\n",
        "    'Major': ['CS', 'Math', 'CS', 'Physics'],  # Nominal\n",
        "    'Year': ['Freshman', 'Sophomore', 'Junior', 'Senior']  # Ordinal\n",
        "})\n",
        "\n",
        "print(\"Student Data:\")\n",
        "print(student_data)\n",
        "\n",
        "# TODO: Exercise 1 - Create label encoding for Grade (A=4, B=3, C=2, D=1, F=0)\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 2 - Create one-hot encoding for Major\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 3 - Create ordinal encoding for Year\n",
        "# Freshman=1, Sophomore=2, Junior=3, Senior=4\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 4 - Combine all encoded features\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oue2PA90BME"
      },
      "source": [
        "---\n",
        "## Part 6: Feature Correlation\n",
        "\n",
        "Understanding relationships between features helps in feature selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIkc9xiz0BME"
      },
      "source": [
        "### 6.1 Pearson Correlation\n",
        "\n",
        "Measures **linear** relationship between two continuous variables.\n",
        "\n",
        "**Range:** -1 (perfect negative) to +1 (perfect positive)\n",
        "\n",
        "**Formula:** $r = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVhanjy20BME"
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix\n",
        "numerical_features = ['Age', 'Income', 'SpendingScore', 'Purchased']\n",
        "correlation_matrix = df_no_outliers[numerical_features].corr()\n",
        "\n",
        "print(\"Correlation Matrix:\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Visualize with heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature Correlation Heatmap', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSc_fBo30BME"
      },
      "outputs": [],
      "source": [
        "# Find highly correlated features\n",
        "threshold = 0.7\n",
        "\n",
        "# Get upper triangle of correlation matrix\n",
        "upper_triangle = correlation_matrix.where(\n",
        "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "\n",
        "# Find features with correlation > threshold\n",
        "high_corr = [(column, row, upper_triangle.loc[row, column])\n",
        "             for column in upper_triangle.columns\n",
        "             for row in upper_triangle.index\n",
        "             if abs(upper_triangle.loc[row, column]) > threshold]\n",
        "\n",
        "if high_corr:\n",
        "    print(f\"\\nHighly correlated feature pairs (|r| > {threshold}):\")\n",
        "    for feat1, feat2, corr in high_corr:\n",
        "        print(f\"{feat1} <-> {feat2}: {corr:.3f}\")\n",
        "else:\n",
        "    print(f\"\\nNo highly correlated features found (threshold={threshold})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT872cAh0BME"
      },
      "outputs": [],
      "source": [
        "# Scatter plot for feature relationships\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Age vs SpendingScore\n",
        "axes[0].scatter(df_no_outliers['Age'], df_no_outliers['SpendingScore'], alpha=0.5)\n",
        "axes[0].set_xlabel('Age')\n",
        "axes[0].set_ylabel('Spending Score')\n",
        "axes[0].set_title(f\"Age vs Spending Score\\nr = {correlation_matrix.loc['Age', 'SpendingScore']:.3f}\")\n",
        "\n",
        "# Income vs SpendingScore\n",
        "axes[1].scatter(df_no_outliers['Income'], df_no_outliers['SpendingScore'], alpha=0.5, color='orange')\n",
        "axes[1].set_xlabel('Income')\n",
        "axes[1].set_ylabel('Spending Score')\n",
        "axes[1].set_title(f\"Income vs Spending Score\\nr = {correlation_matrix.loc['Income', 'SpendingScore']:.3f}\")\n",
        "\n",
        "# Age vs Income\n",
        "axes[2].scatter(df_no_outliers['Age'], df_no_outliers['Income'], alpha=0.5, color='green')\n",
        "axes[2].set_xlabel('Age')\n",
        "axes[2].set_ylabel('Income')\n",
        "axes[2].set_title(f\"Age vs Income\\nr = {correlation_matrix.loc['Age', 'Income']:.3f}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtoeq0-E0BME"
      },
      "source": [
        "### 6.2 Exercise: Correlation Analysis\n",
        "\n",
        "**Task:** Analyze correlations and interpret results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "893fIGTm0BME"
      },
      "outputs": [],
      "source": [
        "# Create test data with known correlations\n",
        "np.random.seed(42)\n",
        "n = 100\n",
        "\n",
        "# Strong positive correlation\n",
        "x1 = np.random.randn(n)\n",
        "y1 = 2 * x1 + np.random.randn(n) * 0.5\n",
        "\n",
        "# Strong negative correlation\n",
        "x2 = np.random.randn(n)\n",
        "y2 = -2 * x2 + np.random.randn(n) * 0.5\n",
        "\n",
        "# No correlation\n",
        "x3 = np.random.randn(n)\n",
        "y3 = np.random.randn(n)\n",
        "\n",
        "test_corr_data = pd.DataFrame({\n",
        "    'X1': x1, 'Y1': y1,\n",
        "    'X2': x2, 'Y2': y2,\n",
        "    'X3': x3, 'Y3': y3\n",
        "})\n",
        "\n",
        "# TODO: Exercise 1 - Calculate correlation between X1 and Y1\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 2 - Calculate correlation between X2 and Y2\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 3 - Calculate correlation between X3 and Y3\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 4 - Create scatter plots for all three pairs\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 5 - Interpret the correlations\n",
        "# Write your interpretation here:\n",
        "# X1-Y1: ...\n",
        "# X2-Y2: ...\n",
        "# X3-Y3: ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0DHRHs90BME"
      },
      "source": [
        "---\n",
        "## Part 7: Handling Imbalanced Datasets\n",
        "\n",
        "Imbalanced datasets occur when one class significantly outnumbers another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7lPu4LL0BME"
      },
      "source": [
        "### 7.1 Detecting Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzUf_meY0BME"
      },
      "outputs": [],
      "source": [
        "# Check class distribution\n",
        "print(\"Class Distribution:\")\n",
        "print(df_no_outliers['Purchased'].value_counts())\n",
        "\n",
        "print(\"\\nClass Proportions:\")\n",
        "print(df_no_outliers['Purchased'].value_counts(normalize=True))\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Count plot\n",
        "df_no_outliers['Purchased'].value_counts().plot(kind='bar', ax=axes[0], color=['#ff6b6b', '#4ecdc4'])\n",
        "axes[0].set_title('Class Distribution')\n",
        "axes[0].set_xlabel('Class')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xticklabels(['Not Purchased', 'Purchased'], rotation=0)\n",
        "\n",
        "# Pie chart\n",
        "df_no_outliers['Purchased'].value_counts().plot(kind='pie', ax=axes[1],\n",
        "                                                  labels=['Not Purchased', 'Purchased'],\n",
        "                                                  autopct='%1.1f%%', colors=['#ff6b6b', '#4ecdc4'])\n",
        "axes[1].set_title('Class Proportion')\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3-NlG9i0BME"
      },
      "source": [
        "### 7.2 Handling Imbalance - Undersampling\n",
        "\n",
        "Randomly remove samples from the majority class to match the minority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm8Re3vd0BME"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Separate majority and minority classes\n",
        "df_majority = df_no_outliers[df_no_outliers['Purchased'] == 0]\n",
        "df_minority = df_no_outliers[df_no_outliers['Purchased'] == 1]\n",
        "\n",
        "print(f\"Majority class size: {len(df_majority)}\")\n",
        "print(f\"Minority class size: {len(df_minority)}\")\n",
        "\n",
        "# Downsample majority class\n",
        "df_majority_downsampled = resample(df_majority,\n",
        "                                   replace=False,\n",
        "                                   n_samples=len(df_minority),\n",
        "                                   random_state=42)\n",
        "\n",
        "# Combine minority class with downsampled majority class\n",
        "df_balanced_under = pd.concat([df_majority_downsampled, df_minority])\n",
        "\n",
        "print(f\"\\nBalanced dataset size: {len(df_balanced_under)}\")\n",
        "print(\"\\nNew class distribution:\")\n",
        "print(df_balanced_under['Purchased'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT1H85sS0BME"
      },
      "source": [
        "### 7.3 Handling Imbalance - Oversampling\n",
        "\n",
        "Randomly duplicate samples from the minority class to match the majority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80qRJpt-0BME"
      },
      "outputs": [],
      "source": [
        "# Upsample minority class\n",
        "df_minority_upsampled = resample(df_minority,\n",
        "                                 replace=True,\n",
        "                                 n_samples=len(df_majority),\n",
        "                                 random_state=42)\n",
        "\n",
        "# Combine majority class with upsampled minority class\n",
        "df_balanced_over = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "print(f\"Balanced dataset size: {len(df_balanced_over)}\")\n",
        "print(\"\\nNew class distribution:\")\n",
        "print(df_balanced_over['Purchased'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9lCJeae0BME"
      },
      "outputs": [],
      "source": [
        "# Compare all three approaches\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Original\n",
        "df_no_outliers['Purchased'].value_counts().plot(kind='bar', ax=axes[0], color=['#ff6b6b', '#4ecdc4'])\n",
        "axes[0].set_title(f'Original (n={len(df_no_outliers)})')\n",
        "axes[0].set_xlabel('Class')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xticklabels(['Not Purchased', 'Purchased'], rotation=0)\n",
        "\n",
        "# Undersampled\n",
        "df_balanced_under['Purchased'].value_counts().plot(kind='bar', ax=axes[1], color=['#ff6b6b', '#4ecdc4'])\n",
        "axes[1].set_title(f'Undersampled (n={len(df_balanced_under)})')\n",
        "axes[1].set_xlabel('Class')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].set_xticklabels(['Not Purchased', 'Purchased'], rotation=0)\n",
        "\n",
        "# Oversampled\n",
        "df_balanced_over['Purchased'].value_counts().plot(kind='bar', ax=axes[2], color=['#ff6b6b', '#4ecdc4'])\n",
        "axes[2].set_title(f'Oversampled (n={len(df_balanced_over)})')\n",
        "axes[2].set_xlabel('Class')\n",
        "axes[2].set_ylabel('Count')\n",
        "axes[2].set_xticklabels(['Not Purchased', 'Purchased'], rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTvxxq7k0BME"
      },
      "source": [
        "### 7.4 Exercise: Handling Imbalanced Data\n",
        "\n",
        "**Task:** Practice balancing an imbalanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4LAw3um0BME"
      },
      "outputs": [],
      "source": [
        "# Create highly imbalanced dataset\n",
        "np.random.seed(42)\n",
        "imbalanced_data = pd.DataFrame({\n",
        "    'Feature1': np.random.randn(1000),\n",
        "    'Feature2': np.random.randn(1000),\n",
        "    'Class': np.random.choice([0, 1], 1000, p=[0.95, 0.05])\n",
        "})\n",
        "\n",
        "print(\"Original class distribution:\")\n",
        "print(imbalanced_data['Class'].value_counts())\n",
        "print(\"\\nClass proportions:\")\n",
        "print(imbalanced_data['Class'].value_counts(normalize=True))\n",
        "\n",
        "# TODO: Exercise 1 - Apply undersampling\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 2 - Apply oversampling\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 3 - Compare the sizes of original, undersampled, and oversampled datasets\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# TODO: Exercise 4 - Visualize the class distributions for all three datasets\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jo8eBxY0BME"
      },
      "source": [
        "---\n",
        "## Part 8: Complete Preprocessing Pipeline\n",
        "\n",
        "Let's put it all together!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3ZnWEr60BME"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(df, handle_outliers='remove', scale_method='standard', balance_method=None):\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Input data\n",
        "    handle_outliers : str\n",
        "        'remove', 'cap', or None\n",
        "    scale_method : str\n",
        "        'standard', 'minmax', or None\n",
        "    balance_method : str\n",
        "        'undersample', 'oversample', or None\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame\n",
        "        Preprocessed data\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    print(\"=== Starting Preprocessing Pipeline ===\")\n",
        "    print(f\"Initial shape: {df_processed.shape}\")\n",
        "\n",
        "    # Step 1: Handle missing values\n",
        "    print(\"\\n1. Handling missing values...\")\n",
        "    numerical_cols = df_processed.select_dtypes(include=['int64', 'float64']).columns\n",
        "    for col in numerical_cols:\n",
        "        if df_processed[col].isnull().sum() > 0:\n",
        "            df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
        "    print(f\"   Missing values after imputation: {df_processed.isnull().sum().sum()}\")\n",
        "\n",
        "    # Step 2: Handle outliers\n",
        "    if handle_outliers:\n",
        "        print(f\"\\n2. Handling outliers ({handle_outliers} method)...\")\n",
        "        for col in numerical_cols:\n",
        "            if col not in ['CustomerID', 'Purchased']:\n",
        "                outliers = detect_outliers_iqr(df_processed[col])\n",
        "                outlier_count = outliers.sum()\n",
        "\n",
        "                if handle_outliers == 'remove' and outlier_count > 0:\n",
        "                    df_processed = df_processed[~outliers]\n",
        "                    print(f\"   {col}: Removed {outlier_count} outliers\")\n",
        "                elif handle_outliers == 'cap' and outlier_count > 0:\n",
        "                    Q1 = df_processed[col].quantile(0.25)\n",
        "                    Q3 = df_processed[col].quantile(0.75)\n",
        "                    IQR = Q3 - Q1\n",
        "                    lower = Q1 - 1.5 * IQR\n",
        "                    upper = Q3 + 1.5 * IQR\n",
        "                    df_processed[col] = df_processed[col].clip(lower, upper)\n",
        "                    print(f\"   {col}: Capped {outlier_count} outliers\")\n",
        "        print(f\"   Shape after outlier handling: {df_processed.shape}\")\n",
        "\n",
        "    # Step 3: Feature encoding\n",
        "    print(\"\\n3. Encoding categorical features...\")\n",
        "    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_cols:\n",
        "        if col in ['Gender', 'City']:\n",
        "            # One-hot encoding\n",
        "            encoded = pd.get_dummies(df_processed[col], prefix=col, drop_first=True)\n",
        "            df_processed = pd.concat([df_processed, encoded], axis=1)\n",
        "            df_processed.drop(col, axis=1, inplace=True)\n",
        "            print(f\"   {col}: One-hot encoded\")\n",
        "        elif col == 'Education':\n",
        "            # Ordinal encoding\n",
        "            education_map = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\n",
        "            df_processed[col + '_encoded'] = df_processed[col].map(education_map)\n",
        "            df_processed.drop(col, axis=1, inplace=True)\n",
        "            print(f\"   {col}: Ordinal encoded\")\n",
        "\n",
        "    # Step 4: Feature scaling\n",
        "    if scale_method:\n",
        "        print(f\"\\n4. Scaling features ({scale_method} method)...\")\n",
        "        scale_cols = [col for col in df_processed.columns\n",
        "                     if col not in ['CustomerID', 'Purchased']\n",
        "                     and df_processed[col].dtype in ['int64', 'float64']]\n",
        "\n",
        "        if scale_method == 'standard':\n",
        "            scaler = StandardScaler()\n",
        "        elif scale_method == 'minmax':\n",
        "            scaler = MinMaxScaler()\n",
        "\n",
        "        df_processed[scale_cols] = scaler.fit_transform(df_processed[scale_cols])\n",
        "        print(f\"   Scaled {len(scale_cols)} features\")\n",
        "\n",
        "    # Step 5: Handle class imbalance\n",
        "    if balance_method and 'Purchased' in df_processed.columns:\n",
        "        print(f\"\\n5. Balancing classes ({balance_method} method)...\")\n",
        "        majority = df_processed[df_processed['Purchased'] == 0]\n",
        "        minority = df_processed[df_processed['Purchased'] == 1]\n",
        "\n",
        "        print(f\"   Before: Majority={len(majority)}, Minority={len(minority)}\")\n",
        "\n",
        "        if balance_method == 'undersample':\n",
        "            majority = resample(majority, n_samples=len(minority), random_state=42)\n",
        "        elif balance_method == 'oversample':\n",
        "            minority = resample(minority, replace=True, n_samples=len(majority), random_state=42)\n",
        "\n",
        "        df_processed = pd.concat([majority, minority])\n",
        "        print(f\"   After: {df_processed['Purchased'].value_counts().to_dict()}\")\n",
        "\n",
        "    print(f\"\\n=== Preprocessing Complete ===\")\n",
        "    print(f\"Final shape: {df_processed.shape}\")\n",
        "\n",
        "    return df_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnQtCGiZ0BMF"
      },
      "outputs": [],
      "source": [
        "# Apply complete preprocessing pipeline\n",
        "df_final = preprocess_data(\n",
        "    df,\n",
        "    handle_outliers='remove',\n",
        "    scale_method='standard',\n",
        "    balance_method='oversample'\n",
        ")\n",
        "\n",
        "print(\"\\nFinal preprocessed data:\")\n",
        "df_final.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TibyjDpp0BMF"
      },
      "source": [
        "### 8.1 Final Exercise: Complete Preprocessing Pipeline\n",
        "\n",
        "**Task:** Create your own preprocessing pipeline from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZA_GXfQ0BMF"
      },
      "outputs": [],
      "source": [
        "# Load the original data again\n",
        "df_exercise = pd.read_csv('customer_data.csv')\n",
        "\n",
        "# TODO: Complete the following preprocessing steps\n",
        "\n",
        "# Step 1: Explore the data\n",
        "# - Check shape, dtypes, missing values, basic statistics\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "# - Use appropriate imputation for numerical features\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# Step 3: Detect and handle outliers\n",
        "# - Use IQR method\n",
        "# - Choose remove or cap strategy\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# Step 4: Encode categorical features\n",
        "# - Use appropriate encoding for each feature\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# Step 5: Scale numerical features\n",
        "# - Choose standardization or normalization\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# Step 6: Analyze feature correlations\n",
        "# - Create correlation matrix and heatmap\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# Step 7: Handle class imbalance\n",
        "# - Choose undersampling or oversampling\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# Step 8: Save the preprocessed data\n",
        "# - Save to CSV file\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6hQCzUv0BMF"
      },
      "source": [
        "---\n",
        "## Conclusion and Summary\n",
        "\n",
        "In this lab, you learned:\n",
        "\n",
        "1. **Pandas Basics**\n",
        "   - Creating and manipulating DataFrames\n",
        "   - Selecting and filtering data\n",
        "   - Basic data exploration\n",
        "\n",
        "2. **Data Loading and Exploration**\n",
        "   - Loading CSV files\n",
        "   - Identifying data types\n",
        "   - Basic EDA techniques\n",
        "\n",
        "3. **Missing Value Handling**\n",
        "   - Detection methods\n",
        "   - Imputation strategies (mean, median, mode)\n",
        "   - When to use each method\n",
        "\n",
        "4. **Outlier Detection and Handling**\n",
        "   - Z-score method\n",
        "   - IQR method\n",
        "   - Removal vs capping strategies\n",
        "\n",
        "5. **Feature Scaling**\n",
        "   - Standardization (Z-score normalization)\n",
        "   - Min-Max normalization\n",
        "   - When to use each method\n",
        "\n",
        "6. **Feature Encoding**\n",
        "   - Label encoding for ordinal data\n",
        "   - One-hot encoding for nominal data\n",
        "   - Avoiding common pitfalls\n",
        "\n",
        "7. **Feature Correlation**\n",
        "   - Pearson correlation\n",
        "   - Correlation matrices and heatmaps\n",
        "   - Interpreting correlations\n",
        "\n",
        "8. **Imbalanced Dataset Handling**\n",
        "   - Undersampling\n",
        "   - Oversampling\n",
        "   - Trade-offs between methods\n",
        "\n",
        "### Key Takeaways:\n",
        "- Always explore your data first!\n",
        "- Choose preprocessing techniques based on your data characteristics\n",
        "- Document your preprocessing decisions\n",
        "- Save your preprocessing pipeline for consistent application\n",
        "\n",
        "### Next Steps:\n",
        "- Practice with different datasets\n",
        "- Learn advanced techniques (SMOTE, feature engineering)\n",
        "- Apply preprocessing before training ML models\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNI4qGPX0BMF"
      },
      "source": [
        "## Submission Instructions\n",
        "\n",
        "1. Complete all exercises marked with \"TODO\"\n",
        "2. Make sure all cells run without errors\n",
        "3. Save your notebook with filename: `Lab_2_Data_Preprocessing_YourFullName.ipynb`\n",
        "4. Submit via Google Classroom before the deadline\n",
        "\n",
        "**Grading Criteria:**\n",
        "- Completeness of exercises (40%)\n",
        "- Code correctness (30%)\n",
        "- Understanding and interpretation (20%)\n",
        "- Code documentation and clarity (10%)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_w9UuRjQ3Ujr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}